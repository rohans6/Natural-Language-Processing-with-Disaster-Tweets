{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras import layers\n",
    "import string\n",
    "import re\n",
    "import wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"../input/nlp-getting-started/train.csv\")\n",
    "test=pd.read_csv(\"../input/nlp-getting-started/test.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['keyword'] = train['keyword'].fillna('unknown')\n",
    "test['keyword'] = test['keyword'].fillna('unknown')\n",
    "\n",
    "#add keyword to tweets\n",
    "train['text'] = train['text'] + ' ' + train['keyword']\n",
    "test['text'] = test['text'] + ' ' + test['keyword']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total=train.append(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(x):\n",
    "    return x.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "#remove stopwords\n",
    "def remove_stopwords(x):\n",
    "    return ' '.join([i for i in x.split() if i not in wordcloud.STOPWORDS])\n",
    "\n",
    "#remove words less than 4 \n",
    "def remove_less_than(x):\n",
    "    return ' '.join([i for i in x.split() if len(i) > 3])\n",
    "\n",
    "#remove words with non-alphabet characters\n",
    "def remove_non_alphabet(x):\n",
    "    return ' '.join([i for i in x.split() if i.isalpha()])\n",
    "\n",
    "def strip_all_entities(x):\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install autocorrect\n",
    "from autocorrect import Speller \n",
    "\n",
    "#create function to spell check strings\n",
    "def spell_check(x):\n",
    "    spell = Speller(lang='en')\n",
    "    return \" \".join([spell(i) for i in x.split()])\n",
    "\n",
    "#showcase spellcheck \n",
    "mispelled = 'Pleaze spelcheck this sentince'\n",
    "spell_check(mispelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total['text'] =total['text'].apply(lambda x: x.lower())\n",
    "total['text'] =total['text'].apply(lambda x: re.sub(r'https?://\\S+|www\\.\\S+', '', x, flags = re.MULTILINE))\n",
    "total['text']=total['text'].apply(remove_punctuation)\n",
    "total['text']=total['text'].apply(remove_non_alphabet)\n",
    "total['text']=total['text'].apply(remove_less_than)\n",
    "total['text']=total['text'].apply(remove_stopwords)\n",
    "total['text']=total['text'].apply(spell_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total['text']=total['text'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text'] = test['text'].apply(lambda x: x.lower())\n",
    "test['text'] = test['text'].apply(lambda x: re.sub(r'https?://\\S+|www\\.\\S+', '', x, flags = re.MULTILINE))\n",
    "test['text']=test['text'].apply(remove_punctuation)\n",
    "test['text']=test['text'].apply(remove_non_alphabet)\n",
    "test['text']=test['text'].apply(remove_less_than)\n",
    "test['text']=test['text'].apply(remove_stopwords)\n",
    "test['text']=test['text'].apply(spell_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(tweet):\n",
    "\n",
    "    #correct some acronyms while we are at it\n",
    "    tweet = re.sub(r\"tnwx\", \"Tennessee Weather\", tweet)\n",
    "    tweet = re.sub(r\"azwx\", \"Arizona Weather\", tweet)  \n",
    "    tweet = re.sub(r\"alwx\", \"Alabama Weather\", tweet)\n",
    "    tweet = re.sub(r\"wordpressdotcom\", \"wordpress\", tweet)      \n",
    "    tweet = re.sub(r\"gawx\", \"Georgia Weather\", tweet)  \n",
    "    tweet = re.sub(r\"scwx\", \"South Carolina Weather\", tweet)  \n",
    "    tweet = re.sub(r\"cawx\", \"California Weather\", tweet)\n",
    "    tweet = re.sub(r\"usNWSgov\", \"United States National Weather Service\", tweet) \n",
    "    tweet = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", tweet)\n",
    "    tweet = re.sub(r\"okwx\", \"Oklahoma City Weather\", tweet)\n",
    "    tweet = re.sub(r\"arwx\", \"Arkansas Weather\", tweet)  \n",
    "    tweet = re.sub(r\"lmao\", \"laughing my ass off\", tweet)  \n",
    "    tweet = re.sub(r\"amirite\", \"am I right\", tweet)\n",
    "    \n",
    "    #and some typos/abbreviations\n",
    "    tweet = re.sub(r\"w/e\", \"whatever\", tweet)\n",
    "    tweet = re.sub(r\"w/\", \"with\", tweet)\n",
    "    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n",
    "    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\n",
    "    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n",
    "    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n",
    "    tweet = re.sub(r\"<3\", \"love\", tweet)\n",
    "    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\n",
    "    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n",
    "    tweet = re.sub(r\"WindStorm\", \"Wind Storm\", tweet)\n",
    "    tweet = re.sub(r\"16yr\", \"16 year\", tweet)\n",
    "    tweet = re.sub(r\"TRAUMATISED\", \"traumatized\", tweet)\n",
    "    \n",
    "    #hashtags and usernames\n",
    "    tweet = re.sub(r\"IranDeal\", \"Iran Deal\", tweet)\n",
    "    tweet = re.sub(r\"ArianaGrande\", \"Ariana Grande\", tweet)\n",
    "    tweet = re.sub(r\"camilacabello97\", \"camila cabello\", tweet) \n",
    "    tweet = re.sub(r\"RondaRousey\", \"Ronda Rousey\", tweet)     \n",
    "    tweet = re.sub(r\"MTVHottest\", \"MTV Hottest\", tweet)\n",
    "    tweet = re.sub(r\"TrapMusic\", \"Trap Music\", tweet)\n",
    "    tweet = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", tweet)\n",
    "    tweet = re.sub(r\"PantherAttack\", \"Panther Attack\", tweet)\n",
    "    tweet = re.sub(r\"StrategicPatience\", \"Strategic Patience\", tweet)\n",
    "    tweet = re.sub(r\"socialnews\", \"social news\", tweet)\n",
    "    tweet = re.sub(r\"IDPs:\", \"Internally Displaced People :\", tweet)\n",
    "    tweet = re.sub(r\"ArtistsUnited\", \"Artists United\", tweet)\n",
    "    tweet = re.sub(r\"ClaytonBryant\", \"Clayton Bryant\", tweet)\n",
    "    tweet = re.sub(r\"jimmyfallon\", \"jimmy fallon\", tweet)\n",
    "    tweet = re.sub(r\"justinbieber\", \"justin bieber\", tweet)  \n",
    "    tweet = re.sub(r\"Time2015\", \"Time 2015\", tweet)\n",
    "    tweet = re.sub(r\"djicemoon\", \"dj icemoon\", tweet)\n",
    "    tweet = re.sub(r\"LivingSafely\", \"Living Safely\", tweet)\n",
    "    tweet = re.sub(r\"FIFA16\", \"Fifa 2016\", tweet)\n",
    "    tweet = re.sub(r\"thisiswhywecanthavenicethings\", \"this is why we cannot have nice things\", tweet)\n",
    "    tweet = re.sub(r\"bbcnews\", \"bbc news\", tweet)\n",
    "    tweet = re.sub(r\"UndergroundRailraod\", \"Underground Railraod\", tweet)\n",
    "    tweet = re.sub(r\"c4news\", \"c4 news\", tweet)\n",
    "    tweet = re.sub(r\"MUDSLIDE\", \"mudslide\", tweet)\n",
    "    tweet = re.sub(r\"NoSurrender\", \"No Surrender\", tweet)\n",
    "    tweet = re.sub(r\"NotExplained\", \"Not Explained\", tweet)\n",
    "    tweet = re.sub(r\"greatbritishbakeoff\", \"great british bake off\", tweet)\n",
    "    tweet = re.sub(r\"LondonFire\", \"London Fire\", tweet)\n",
    "    tweet = re.sub(r\"KOTAWeather\", \"KOTA Weather\", tweet)\n",
    "    tweet = re.sub(r\"LuchaUnderground\", \"Lucha Underground\", tweet)\n",
    "    tweet = re.sub(r\"KOIN6News\", \"KOIN 6 News\", tweet)\n",
    "    tweet = re.sub(r\"LiveOnK2\", \"Live On K2\", tweet)\n",
    "    tweet = re.sub(r\"9NewsGoldCoast\", \"9 News Gold Coast\", tweet)\n",
    "    tweet = re.sub(r\"nikeplus\", \"nike plus\", tweet)\n",
    "    tweet = re.sub(r\"david_cameron\", \"David Cameron\", tweet)\n",
    "    tweet = re.sub(r\"peterjukes\", \"Peter Jukes\", tweet)\n",
    "    tweet = re.sub(r\"MikeParrActor\", \"Michael Parr\", tweet)\n",
    "    tweet = re.sub(r\"4PlayThursdays\", \"Foreplay Thursdays\", tweet)\n",
    "    tweet = re.sub(r\"TGF2015\", \"Tontitown Grape Festival\", tweet)\n",
    "    tweet = re.sub(r\"realmandyrain\", \"Mandy Rain\", tweet)\n",
    "    tweet = re.sub(r\"GraysonDolan\", \"Grayson Dolan\", tweet)\n",
    "    tweet = re.sub(r\"ApolloBrown\", \"Apollo Brown\", tweet)\n",
    "    tweet = re.sub(r\"saddlebrooke\", \"Saddlebrooke\", tweet)\n",
    "    tweet = re.sub(r\"TontitownGrape\", \"Tontitown Grape\", tweet)\n",
    "    tweet = re.sub(r\"AbbsWinston\", \"Abbs Winston\", tweet)\n",
    "    tweet = re.sub(r\"ShaunKing\", \"Shaun King\", tweet)\n",
    "    tweet = re.sub(r\"MeekMill\", \"Meek Mill\", tweet)\n",
    "    tweet = re.sub(r\"TornadoGiveaway\", \"Tornado Giveaway\", tweet)\n",
    "    tweet = re.sub(r\"GRupdates\", \"GR updates\", tweet)\n",
    "    tweet = re.sub(r\"SouthDowns\", \"South Downs\", tweet)\n",
    "    tweet = re.sub(r\"braininjury\", \"brain injury\", tweet)\n",
    "    tweet = re.sub(r\"auspol\", \"Australian politics\", tweet)\n",
    "    tweet = re.sub(r\"PlannedParenthood\", \"Planned Parenthood\", tweet)\n",
    "    tweet = re.sub(r\"calgaryweather\", \"Calgary Weather\", tweet)\n",
    "    tweet = re.sub(r\"weallheartonedirection\", \"we all heart one direction\", tweet)\n",
    "    tweet = re.sub(r\"edsheeran\", \"Ed Sheeran\", tweet)\n",
    "    tweet = re.sub(r\"TrueHeroes\", \"True Heroes\", tweet)\n",
    "    tweet = re.sub(r\"ComplexMag\", \"Complex Magazine\", tweet)\n",
    "    tweet = re.sub(r\"TheAdvocateMag\", \"The Advocate Magazine\", tweet)\n",
    "    tweet = re.sub(r\"CityofCalgary\", \"City of Calgary\", tweet)\n",
    "    tweet = re.sub(r\"EbolaOutbreak\", \"Ebola Outbreak\", tweet)\n",
    "    tweet = re.sub(r\"SummerFate\", \"Summer Fate\", tweet)\n",
    "    tweet = re.sub(r\"RAmag\", \"Royal Academy Magazine\", tweet)\n",
    "    tweet = re.sub(r\"offers2go\", \"offers to go\", tweet)\n",
    "    tweet = re.sub(r\"ModiMinistry\", \"Modi Ministry\", tweet)\n",
    "    tweet = re.sub(r\"TAXIWAYS\", \"taxi ways\", tweet)\n",
    "    tweet = re.sub(r\"Calum5SOS\", \"Calum Hood\", tweet)\n",
    "    tweet = re.sub(r\"JamesMelville\", \"James Melville\", tweet)\n",
    "    tweet = re.sub(r\"JamaicaObserver\", \"Jamaica Observer\", tweet)\n",
    "    tweet = re.sub(r\"TweetLikeItsSeptember11th2001\", \"Tweet like it is september 11th 2001\", tweet)\n",
    "    tweet = re.sub(r\"cbplawyers\", \"cbp lawyers\", tweet)\n",
    "    tweet = re.sub(r\"fewmoretweets\", \"few more tweets\", tweet)\n",
    "    tweet = re.sub(r\"BlackLivesMatter\", \"Black Lives Matter\", tweet)\n",
    "    tweet = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", tweet)\n",
    "    tweet = re.sub(r\"onlinecommunities\", \"online communities\", tweet)\n",
    "    tweet = re.sub(r\"humanconsumption\", \"human consumption\", tweet)\n",
    "    tweet = re.sub(r\"Typhoon-Devastated\", \"Typhoon Devastated\", tweet)\n",
    "    tweet = re.sub(r\"Meat-Loving\", \"Meat Loving\", tweet)\n",
    "    tweet = re.sub(r\"facialabuse\", \"facial abuse\", tweet)\n",
    "    tweet = re.sub(r\"LakeCounty\", \"Lake County\", tweet)\n",
    "    tweet = re.sub(r\"BeingAuthor\", \"Being Author\", tweet)\n",
    "    tweet = re.sub(r\"withheavenly\", \"with heavenly\", tweet)\n",
    "    tweet = re.sub(r\"thankU\", \"thank you\", tweet)\n",
    "    tweet = re.sub(r\"iTunesMusic\", \"iTunes Music\", tweet)\n",
    "    tweet = re.sub(r\"OffensiveContent\", \"Offensive Content\", tweet)\n",
    "    tweet = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", tweet)\n",
    "    tweet = re.sub(r\"HarryBeCareful\", \"Harry Be Careful\", tweet)\n",
    "    tweet = re.sub(r\"NASASolarSystem\", \"NASA Solar System\", tweet)\n",
    "    tweet = re.sub(r\"animalrescue\", \"animal rescue\", tweet)\n",
    "    tweet = re.sub(r\"KurtSchlichter\", \"Kurt Schlichter\", tweet)\n",
    "    tweet = re.sub(r\"Throwingknifes\", \"Throwing knives\", tweet)\n",
    "    tweet = re.sub(r\"GodsLove\", \"God's Love\", tweet)\n",
    "    tweet = re.sub(r\"bookboost\", \"book boost\", tweet)\n",
    "    tweet = re.sub(r\"ibooklove\", \"I book love\", tweet)\n",
    "    tweet = re.sub(r\"NestleIndia\", \"Nestle India\", tweet)\n",
    "    tweet = re.sub(r\"realDonaldTrump\", \"Donald Trump\", tweet)\n",
    "    tweet = re.sub(r\"DavidVonderhaar\", \"David Vonderhaar\", tweet)\n",
    "    tweet = re.sub(r\"CecilTheLion\", \"Cecil The Lion\", tweet)\n",
    "    tweet = re.sub(r\"weathernetwork\", \"weather network\", tweet)\n",
    "    tweet = re.sub(r\"GOPDebate\", \"GOP Debate\", tweet)\n",
    "    tweet = re.sub(r\"RickPerry\", \"Rick Perry\", tweet)\n",
    "    tweet = re.sub(r\"frontpage\", \"front page\", tweet)\n",
    "    tweet = re.sub(r\"NewsInTweets\", \"News In Tweets\", tweet)\n",
    "    tweet = re.sub(r\"ViralSpell\", \"Viral Spell\", tweet)\n",
    "    tweet = re.sub(r\"til_now\", \"until now\", tweet)\n",
    "    tweet = re.sub(r\"volcanoinRussia\", \"volcano in Russia\", tweet)\n",
    "    tweet = re.sub(r\"ZippedNews\", \"Zipped News\", tweet)\n",
    "    tweet = re.sub(r\"MicheleBachman\", \"Michele Bachman\", tweet)\n",
    "    tweet = re.sub(r\"53inch\", \"53 inch\", tweet)\n",
    "    tweet = re.sub(r\"KerrickTrial\", \"Kerrick Trial\", tweet)\n",
    "    tweet = re.sub(r\"abstorm\", \"Alberta Storm\", tweet)\n",
    "    tweet = re.sub(r\"Beyhive\", \"Beyonce hive\", tweet)\n",
    "    tweet = re.sub(r\"RockyFire\", \"Rocky Fire\", tweet)\n",
    "    tweet = re.sub(r\"Listen/Buy\", \"Listen / Buy\", tweet)\n",
    "    tweet = re.sub(r\"ArtistsUnited\", \"Artists United\", tweet)\n",
    "    tweet = re.sub(r\"ENGvAUS\", \"England vs Australia\", tweet)\n",
    "    tweet = re.sub(r\"ScottWalker\", \"Scott Walker\", tweet)\n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total.text=total.text.apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(total.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(total.text)\n",
    "total.text=tokenizer.texts_to_sequences(total.text)\n",
    "vocab=tokenizer.word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pad_sequences(total.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=data[:len(train)]\n",
    "X_test=data[len(train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=total.target[:len(train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence  import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_val,y_train,y_val=train_test_split(X_train,y_train,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_vec={}\n",
    "words_to_idx={}\n",
    "idx_to_words={}\n",
    "words=set()\n",
    "with open('../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt') as f:\n",
    "    data=f.readlines()\n",
    "for line in data:\n",
    "    line=line.strip().split()\n",
    "    word=line[0]\n",
    "    words.add(word)\n",
    "    words_to_vec[word]=np.array(line[1:],dtype=np.float32)\n",
    "index=1\n",
    "for word in sorted(words):\n",
    "    words_to_idx[word]=index\n",
    "    idx_to_words[index]=word\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_Embedding_layer():\n",
    "    vocab_size=len(vocab)+1\n",
    "    dim=200\n",
    "    emd_matrix=np.zeros((vocab_size,dim))\n",
    "    for (word,index) in tokenizer.word_index.items():\n",
    "        word_vector=words_to_vec.get(word)\n",
    "        if word_vector is not None:\n",
    "            emd_matrix[index,:]=words_to_vec[word]\n",
    "    layer=layers.Embedding(input_dim=vocab_size,output_dim=dim,trainable=False)\n",
    "    layer.build((None,))\n",
    "    layer.set_weights([emd_matrix])\n",
    "    return layer\n",
    "layer=load_pretrained_Embedding_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = layers.LeakyReLU(alpha = 0.01)\n",
    "model=Sequential()\n",
    "model.add(layer)\n",
    "model.add(layers.Bidirectional(layers.LSTM(100,dropout=0.4,recurrent_dropout =0.4)))\n",
    "model.add(layers.Dropout(0.4))\n",
    "model.add(layers.Dense(100,activation=activation))\n",
    "model.add(layers.Dropout(0.4))\n",
    "model.add(layers.Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',metrics=['accuracy'],optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train,batch_size=64,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_padded=tokenizer.texts_to_sequences(test.text)\n",
    "X_test_padded=pad_sequences(X_test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict=model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predict[predict==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predict[predict==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame()\n",
    "df['id']=test.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target']=predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"sub2\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
